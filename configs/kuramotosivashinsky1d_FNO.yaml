seed_everything: 42
trainer:
  default_root_dir: ${oc.env:PDEARENA_OUTPUT_DIR,outputs}
  logger:
    class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: ${trainer.default_root_dir}/tb
      log_graph: False
      default_hp_metric: True
      prefix: ""

  enable_checkpointing: true
  callbacks:
    - class_path: pytorch_lightning.callbacks.Timer
      init_args:
        interval: "epoch"
    - class_path: pytorch_lightning.callbacks.RichModelSummary
      init_args:
        max_depth: -1
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"
    - class_path: pytorch_lightning.callbacks.TQDMProgressBar
      init_args:
        refresh_rate: 1
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: "valid/mse_loss_mean" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        save_top_k: 5 # save k best models (determined by above metric)
        save_last: true # additionally always save model from last epoch
        verbose: false
        dirpath: ${trainer.default_root_dir}/ckpts
        filename: "epoch_{epoch:03d}"
        auto_insert_metric_name: False

  num_nodes: 1
  enable_progress_bar: true
  overfit_batches: 0.0
  check_val_every_n_epoch: 10
  fast_dev_run: false
  max_epochs: 400
  max_steps: -1

  log_every_n_steps: 50
  accelerator: gpu
  sync_batchnorm: false
  precision: 32
  enable_model_summary: true

  num_sanity_val_steps: 2
  reload_dataloaders_every_n_epochs: 0
  detect_anomaly: false

model:
  name: "FNO-1D-128-32m"
  max_num_steps: 1000
  activation: "gelu"
  criterion: mse
  lr: 1e-4
  time_history: 1
  time_future: 1
  time_gap: 0

data:
  task: KuramotoSivashinsky1D
  time_history: 1
  time_future: 1
  time_gap: 0
  pde:
      n_scalar_components: 1
      n_vector_components: 0
      trajlen: 140
      n_spatial_dim: 1

  batch_size: 128
  pin_memory: True
  num_workers: 12
  train_limit_trajectories: -1
  valid_limit_trajectories: -1
  test_limit_trajectories: -1

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-4
lr_scheduler:
  class_path: pdearena.lr_scheduler.LinearWarmupCosineAnnealingLR
  init_args:
    warmup_epochs: 0
    max_epochs: 400
    warmup_start_lr: 1e-8
    eta_min: 1e-6
